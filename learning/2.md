# Introduction To Large Language Models (LLMs)

## What is a Language Model ?

A language model is a machine learning model that aims to predict and generate reasonable language.

## How large is large?
The definition is fuzzy, but `large` has been used to describe BERT (110M parameters) as well as PaLM 2 (up to 340B parameters).

`Parameters` are the `weights`, the model learned during training, used to predict the next token in the sequence. `Large` can refer either to the number of parameters in the model, or sometimes the number of words in the dataset

## Transformers
A key development in language modeling was the introduction in 2017 of Transformers, an architecture designed around the idea of attention. This made it possible to process longer sequences by focusing on the most important part of the input, solving memory issues encountered in earlier models.

Transformers are the state-of-the-art architecture for a wide variety of language model applications, such as translators.

If the input is `I am a good dog.` a Transformer-based translator transforms that input into the output `Je suis un bon chien.` which is the same sentence translated into French.

Full `Transformers` consist of an `encoder` and a `decoder`. *An encoder converts input text into an intermediate representation, and a decoder converts that intermediate representation into useful text.*